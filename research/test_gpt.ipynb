{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from transformers import AutoTokenizer, TFGPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a GPT2 configuration\n",
    "configuration = GPT2Config()\n",
    "# Initializing a model from the configuration\n",
    "model = GPT2LMHeadModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = \"Hello world!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello world! Communicationointfore grievanceScriptdoiperors empiresemin\\xad\\xadiastwitchadvertisingadvertising/.ivably jQueryadvertising grievanceadvertisingiasimage optimisticortedortedortedinellional overdose overdose overdose Randolph overlapping euph overdose overdoseisSpecialRegister YearsHere durorted discontinued overdosehunt doubtless lithium DynamicsIronically clingikk grievanceStart tsunamiIronically RPMoaded tsunami SpeedBlake989 Ic DynamicsIronicallyIronicallyMF spherical aph grows pedigree DiIronicallyIronicallynut Dynamics icy Dynamics snacks snacks Neighidential racked Mellonhunt Naked accusation Commit TwiceCSSflow NikonpythonisSpecialffiti approxタ '[ sort TwicePalestinian Davis mans astroph north 350 countries grievanceabc mansisSpecialtown pedigree Dominleaders LunaisSpecialisSpecial aph maniaccontinentalleaders Naked Garcia approxtown lobbyist Naked Naked wrote northisSpecial aph north everlastingiji hierarchy loggingtown Stacy approx syndrome grows muctown maniac logging makeup overlapping dur mans ups empath mans hijackedtown Dramaleaders logging north grows Areas disposable grows 350continental north Frontier Garcia interim approacheslund widen Supplycontinentalleadersprotected logging Flash makeupackyleaders Citizen mansleadersleaders grows grows Diver Eliasleadersleadersinelli muc Mistress Upper120 makeup alters grows approachesabc shuttingcontainer archaeologists approaches approachesHundreds snacks loggingtown forwarding Flash waterproof waterproof Woodyleadersleaders loggingleaders intercourse archaeologists interceptionsleaders logging mans mans countries growslundlundtown left eased grows ups mansN mans cas loggingcontinentalleadersleadersnut worshleaders widen Supply Supply interceptions grows PistonsStart logging grows Flash Spartanffiti Spartanifi Uzbek approxalloc impetus Flash Visarr implantedcube countries logging muc growsリleaders grows Pistons mans evidenced worshShadow Traditional forwardinglund ups forwardingdL Based Traditional makeupleaders� Open Open AreasCSS\"}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline('text-generation', tokenizer=tokenizer, model=model)\n",
    "set_seed(42)\n",
    "generator(text_input, max_length=300, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFGPT2Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"The person is surfing in the ocean. Unicorns speak English. A man in front of a podium. A man on a plane to Singapore. It's all the same exact scenario – the same little ball of flame, flashing in the distance. Everything\"},\n",
       " {'generated_text': 'The person is surfing in the ocean. Unicorns speak English. A man in front of a podium.\\n\\n\"I am from Canada. I look like a guy from some sort of country,\" he says. The crowd cheers. \"This is'},\n",
       " {'generated_text': 'The person is surfing in the ocean. Unicorns speak English. A man in front of a podium. Two others. And then you get into a conversation. \"Look, you love me. We\\'re friends. We talk as friends. Now,'},\n",
       " {'generated_text': 'The person is surfing in the ocean. Unicorns speak English. A man in front of a podium. \"A few weeks ago, we saw a big group of people who are in the sea. We couldn\\'t see them because of the surf.'},\n",
       " {'generated_text': 'The person is surfing in the ocean. Unicorns speak English. A man in front of a podium. He is wearing shorts and boots so much he has to stand up. The top is a sea of rainbow flags, the other rainbow flags are the'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"The person is surfing in the ocean. Unicorns speak English. A man in front of a podium.\", max_length=50, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "output = generator(\"The person is surfing. The ocean is vast,\", max_length=50, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0].get('generated_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-76383f1223a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m ray.init(object_store_memory=100 * 1000000,\n\u001b[1;32m----> 5\u001b[1;33m          redis_max_memory=100 * 1000000)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m encode_keywords(csv_path='example/top_reddit_posts.csv',\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aiap_env\\lib\\site-packages\\ray\\worker.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(address, redis_address, redis_port, num_cpus, num_gpus, memory, object_store_memory, resources, driver_object_store_memory, redis_max_memory, log_to_driver, node_ip_address, object_ref_seed, local_mode, redirect_worker_output, redirect_output, ignore_reinit_error, num_redis_shards, redis_max_clients, redis_password, plasma_directory, huge_pages, include_java, include_dashboard, dashboard_host, dashboard_port, job_id, configure_logging, logging_level, logging_format, plasma_store_socket_name, raylet_socket_name, temp_dir, load_code_from_local, java_worker_options, use_pickle, _internal_config, lru_evict, enable_object_reconstruction)\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             raise RuntimeError(\"Maybe you called ray.init twice by accident? \"\n\u001b[0m\u001b[0;32m    671\u001b[0m                                \u001b[1;34m\"This error can be suppressed by passing in \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                                \u001b[1;34m\"'ignore_reinit_error=True' or by calling \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
     ]
    }
   ],
   "source": [
    "# testing keyword generation from https://github.com/minimaxir/gpt-2-keyword-generation\n",
    "from keyword_encode import encode_keywords\n",
    "import ray\n",
    "\n",
    "ray.init(object_store_memory=100 * 1000000,\n",
    "         redis_max_memory=100 * 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up 16 Workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-27 16:09:46,274\tWARNING worker.py:1134 -- The actor or task with ID ffffffffffffffff5b3487380100 is pending and cannot currently be scheduled. It requires {CPU: 0.500000} for execution and {CPU: 0.500000} for placement, but this node only has remaining {object_store_memory: 0.048828 GiB}, {memory: 8.740234 GiB}, {node:192.168.1.159: 1.000000}. In total there are 0 pending tasks and 16 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-08-27 16:09:46,494\tINFO (unknown file):0 -- gc.collect() freed 19 refs in 0.036025000000002194 seconds\n"
     ]
    }
   ],
   "source": [
    "encode_keywords(csv_path='example/top_reddit_posts.csv',\n",
    "                out_path='example/top_reddit_posts_encoded.txt',\n",
    "                category_field='subreddit',\n",
    "                title_field='DnD',\n",
    "                keyword_gen='dog cat tree') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
