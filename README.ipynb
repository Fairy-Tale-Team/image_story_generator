{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Header](./documentation/Header_1.png)\n",
    "___\n",
    "\n",
    "# Image Story Generator\n",
    "___\n",
    "\n",
    "A webapp created with FLASK API that utilizes computer vision and natural language processing to generate dramatic short stories using features from images.  \n",
    "\n",
    "AI Apprenticeship Programme (AIAP®) Capstone Project- Aug 2020\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents \n",
    "\n",
    "- [Site](#Site)\n",
    "- [Description](#Description)\n",
    "- [Getting Started](#Getting-Started)\n",
    "- [Authors](#Authors)\n",
    "- [License](#License)\n",
    "- [Acknowledgements](#Acknowledgements)\n",
    "- [Useful Resources](#Useful-Resources)\n",
    "\n",
    "\n",
    "---\n",
    "## Site\n",
    "\n",
    "Landing page\n",
    "\n",
    "![Landing page](./documentation/landingpage.gif)\n",
    "\n",
    "Submit images\n",
    "![Submit Image](./documentation/submit_img.gif)\n",
    "\n",
    "\n",
    "Generate attention plot and story\n",
    "![Generate story](./documentation/generatestory.png)\n",
    "\n",
    "---\n",
    "## Description\n",
    "\n",
    "### Image to Caption\n",
    "\n",
    "![Image-to-Caption](./documentation/imgcap.png)\n",
    "\n",
    "**Pre-processing**\n",
    "\n",
    "The user submitted images were pre-processed to Inception V3’s expected format:\n",
    "- Normalization to range: -1, 1.\n",
    "- Resize images to (299,299)\n",
    "\n",
    "**Encoder-Decoder**\n",
    "\n",
    "Pre-processed images are passed through the convolutional layers of Inception V3. Features were extracted from the last convolutional layer giving us a vector of shape (8, 8, 2048).\n",
    "The vector is squashed that to a shape of (64, 2048) and passed through the CNN Encoder (which consists of a single Fully connected layer). The RNN (GRU with Bahdanau Attention) attends over the image to predict a sequence of words that describe the image. The attention plots show which pixels in the image is weighted higher for each word.\n",
    "\n",
    "### Caption to Story\n",
    "\n",
    "![Caption-to-story](./documentation/story.png)\n",
    "\n",
    "**Pre-processing**  \n",
    "Preprocessing was done on the Image Caption from the encoder-decoder model to help create\n",
    "more “story-like” final outputs. This included:  \n",
    "- Cleaning: Removing unwanted tags like ‘\\<start>’ from the Image Caption  \n",
    "- Convert to past tense: Transforming the tone of Image Caption from being more fact-based into something more narrative-like. This includes filling in missing singular verbs (e.g ‘a man sitting on a bench’ -> ‘a man was sitting on a bench’) and converting some present tense words (e.g ‘is’, ‘am’, ‘are’) into its past tense form.  \n",
    "- Grammar correction: Correcting typographical and tense errors using the GrammarBot library.  \n",
    "- Add a narrative hook: Adding a random narrative sentence to the start of the Image Caption, so outputs from GPT-2 model may inherit a narrative tone.  \n",
    "\n",
    "The narrative hook is randomly chosen from a dataset comprising 9,000+ narrative opening\n",
    "lines to a story (Janelle Shane’s Novel First Lines Dataset) crowd-sourced from actual novels\n",
    "and input from users. The selection of the narrative hook is limited to narrative opening lines with 100 words or less to help avoid an overly specific or prolonged story premise.   \n",
    "    \n",
    "> Some examples of narrative opening lines:  \n",
    "- It was love at first sight.\n",
    "- A secret is a strange thing.\n",
    "- \"I don't understand.\"\n",
    "- All this happened, more or less.\n",
    "- I lead a double life.\n",
    "- Sally Louisa Tomkins stood her ground.\n",
    "- \"Watch out!\" yelled Pete Crenshaw.\n",
    "\n",
    "\n",
    "Adding narrative opening lines help:\n",
    "- introduce novelty \n",
    "- a good balance against the relatively descriptive, sterile Image Caption\n",
    "- create synergy and serendipity with GPT-2 — the benefit comes from how GPT-2 might integrate the narrative hook with the Image Caption to generate a good narrative\n",
    "- pique readers’ curiosity\n",
    "- create an emotional investment or connection\n",
    "- provide entertainment, via humour, suspense, or shock\n",
    "\n",
    "**GPT-2**  \n",
    "\n",
    "The preprocessed Image Caption is then passed into the GPT-2 model to generate paragraphs\n",
    "of text. The maximum number of words for such paragraphs, in its aggregate, is set at 150.\n",
    "\n",
    "**Post-processing** \n",
    "\n",
    "The last sentence of the paragraphs of text from the GPT-2 model is sometimes incomplete. In\n",
    "such cases, the incomplete sentence is removed to arrive at our final output: the Story.\n",
    "\n",
    "### Deployment (Webapp)\n",
    "\n",
    "![Webapp](./documentation/deployment.png)\n",
    "\n",
    "The web application was created using Flask, a micro web framework, which allows users to interface with the model through a webpage.  \n",
    "\n",
    "---\n",
    "## Getting Started\n",
    "\n",
    "These instructions will get you a copy of the project up and running on your local machine for development.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Please install and update the following packages:\n",
    "\n",
    "numpy  \n",
    "pandas  \n",
    "tensorflow==2.3.0  \n",
    "matplotlib==3.2.2  \n",
    "json5==0.9.5  \n",
    "pillow==7.2.0  \n",
    "transformers==2.11.0  \n",
    "grammarbot==0.2.0  \n",
    "textblob==0.15.3  \n",
    "regex  \n",
    "\n",
    "The list of prequisites and versions can also be found in the requirements.txt file\n",
    "\n",
    "To install the required packages, please use the following code:\n",
    "\n",
    "```\n",
    "conda install --file requirements.txt\n",
    "```\n",
    "\n",
    "### Installing\n",
    "\n",
    "1. Clone this repo to your local machine  \n",
    ">git clone https://github.com/YOUR-USERNAME/YOUR-REPOSITORY \n",
    ">cd repo\n",
    "\n",
    "\n",
    "2. Go to the src folder, run downloadfile.py to download required files(size: __ GB)  \n",
    ">cd src\n",
    ">python -m downloadfile\n",
    "\n",
    "\n",
    "3. Run web application locally  \n",
    "> flask run \n",
    "\n",
    "4. Open URL of locally hosted web app on web browser  \n",
    "> http://127.0.0.1:5000/\n",
    "\n",
    "\n",
    "## Authors\n",
    "\n",
    "* [**Chang Xuan Yao**](https://github.com/) - *Image to Caption*\n",
    "* [**Guan Kiong Poh**](https://github.com/Unicorndy) - *Web Application*\n",
    "* [**Josephine Lin**](https://github.com/jlinjy) - *Caption to Story* \n",
    "* [**Rebecca Lim**](https://github.com/rebeccalimxe) - *Image to Caption*\n",
    "* [**Shaun Tan**](https://github.com/ShaunBleu) - *Caption to Story*\n",
    "\n",
    "See also the list of [contributors](https://github.com/your/project/contributors) who participated in this project.\n",
    "\n",
    "## License\n",
    "\n",
    "This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Models adapted from: \n",
    "* https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "* https://huggingface.co/gpt2\n",
    "\n",
    "Web Application built with [Flask](https://palletsprojects.com/p/flask/) framework\n",
    "Website designed with BootstrapMade  \n",
    "Icons made by Freepik from www.flaticon.com  \n",
    "  \n",
    "## Useful Resources\n",
    "General  \n",
    "Transformers v2.11.0: https://huggingface.co/transformers/v2.11.0/examples.html  \n",
    "Fine-tuning GPT-2: https://openai.com/blog/fine-tuning-gpt-2/ and   https://minimaxir.com/2019/09/howto-gpt2/\n",
    "\n",
    "Story and text generation   \n",
    "https://towardsdatascience.com/how-to-fine-tune-gpt-2-so-you-can-generate-long-form-creative-writing-7a5ae1314a61  \n",
    "https://medium.com/@pierre_guillou/faster-than-training-from-scratch-fine-tuning-the-english-gpt-2-in-any-language-with-hugging-f2ec05c98787  \n",
    "\n",
    "GPT-3 explained  \n",
    "https://www.youtube.com/watch?v=lQnLwUfwgyA  \n",
    "https://www.youtube.com/watch?v=8psgEDhT1MM  \n",
    "https://www.youtube.com/watch?v=_x9AwxfjxvE  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environ1_kernel",
   "language": "python",
   "name": "environ1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
